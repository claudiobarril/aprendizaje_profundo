{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudiobarril/aprendizaje_profundo/blob/main/CNN_implementacion_Mejora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wIQ8hjDpdVi"
      },
      "source": [
        "# Importar lo necesario"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHQUjDs12DLW"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from torchsummary import summary # para ver los parametros y tamaños intermedios del modelo\n",
        "from tqdm import tqdm # para graficar la barra de avance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo\n",
        "import torchinfo as torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biK7BQSsgEM0",
        "outputId": "b4d878e1-3b46-482c-f8c9-34969c208792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeJy8fjPn4wi"
      },
      "source": [
        "#### configuramos el `device` acorde al device disponible\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOV9xybtn4I3"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "OoKY2qZRlR71",
        "outputId": "f9754879-0b47-48d8-d6a6-af6eaec32bb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_tH9u082jpZ"
      },
      "source": [
        "\n",
        "#**MNIST data base**\n",
        "# Ejemplo de red neuronal de convolución (CNN)\n",
        "\n",
        "Vamos a usar la base de datos de MNIST ([ver fuente](http://yann.lecun.com/exdb/mnist/)) para entrenar una CNN que identifique números escritos a mano.\n",
        "\n",
        "Para esto necesitamos:\n",
        "\n",
        "\n",
        "1.   Cargar la base de datos.\n",
        "2.   Ver que la base de datos esté ok.\n",
        "3.   Construir nuestra CNN.\n",
        "4. Ver que las dimensiones de la red sean consistentes.\n",
        "4.   Definir funciones necesarias (de entrenamiento, de costo, etc.).\n",
        "5. Entrenar la red.\n",
        "6. Ver que funcione.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nQ-MLk6Do8e"
      },
      "source": [
        "## 1. Cargar base de datos\n",
        "\n",
        "De la documentación, ver:\n",
        "\n",
        "\n",
        "Transformación `torchvision.transforms.ToTensor()`\n",
        "\n",
        "```\n",
        "... Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]...\n",
        "```\n",
        "\n",
        "Transformación `Normalize`\n",
        "\n",
        "```\n",
        "... Normalize a tensor image with mean and standard deviation. ...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JvzatGF4e0W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3cc4df6-325d-4244-d217-4cab912a078b"
      },
      "source": [
        "# primero creamos el dataset\n",
        "train_dataset = torchvision.datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=torchvision.transforms.Compose([\n",
        "                            torchvision.transforms.ToTensor(),#<---------------- escala entre 0 y 1; pasa a tensor; poner canal en 1ra dim\n",
        "                            torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
        "                            ])\n",
        "                      )\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST('../data', train=False,\n",
        "                   transform=torchvision.transforms.Compose([\n",
        "                        torchvision.transforms.ToTensor(), #<------------------- escala entre 0 y 1; pasa a tensor; poner canal en 1ra dim\n",
        "                        torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
        "                        ])\n",
        "                     )\n",
        "\n",
        "# ahora el dataloader\n",
        "dataloader = {\n",
        "    'train': torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True),\n",
        "    'test': torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, pin_memory=True)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 17.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 482kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.80MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 10.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oikthAE4Dteb"
      },
      "source": [
        "## 2. Ver que la base de datos esté OK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyq2UFIl-Qjy",
        "outputId": "47636ff7-bb1d-4502-9ae1-0216a26ce8fe"
      },
      "source": [
        "print(type(dataloader))\n",
        "print(type(dataloader['train']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "<class 'torch.utils.data.dataloader.DataLoader'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ver imagen and label del dataloader (dataloader -> una herramienta para hacer batches de datasets)\n",
        "train_features, train_labels = next(iter(dataloader['train']))"
      ],
      "metadata": {
        "id": "3dVPXQRch4xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2fs6Qdivs1H",
        "outputId": "3d040afe-84ab-43e5-aa6d-a38ff661fe8b"
      },
      "source": [
        "# verifico sus dimensiones\n",
        "print(f\"Tamaño del batch de feature (input / imagen): {train_features.size()}\")\n",
        "print(f\"Tamaño del batch del label (clase / etiqueta): {train_labels.size()}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño del batch de feature (input / imagen): torch.Size([64, 1, 28, 28])\n",
            "Tamaño del batch del label (clase / etiqueta): torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# tomo 1 imagen para poder visualizarla\n",
        "# y verifico sus dimensiones\n",
        "\n",
        "img = train_features[5]\n",
        "print('tamaño de 1 imagen: ', img.shape)\n",
        "# le QUITO 1 dimension (la del tamaño del batch) para poder graficar\n",
        "img = img.squeeze()\n",
        "print('tamaño de 1 imagen DESPUES de squeeze: ', img.shape)\n",
        "label = train_labels[5]\n",
        "\n",
        "# ploteo esa imagen\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.show()\n",
        "print(f\"Label: {label}\")"
      ],
      "metadata": {
        "id": "rfK_dXQdI2C6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "outputId": "e2b6f450-682d-421b-854a-4833312688b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tamaño de 1 imagen:  torch.Size([1, 28, 28])\n",
            "tamaño de 1 imagen DESPUES de squeeze:  torch.Size([28, 28])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('pixel [0,0]: ',img[0][0])\n",
        "print('pixel maximo: ', torch.max(img))\n",
        "print('pixel minimo: ', torch.min(img))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFnOkkTgYTHV",
        "outputId": "2fdc6a31-9916-48f6-fe45-b231ff8f5eba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pixel [0,0]:  tensor(-0.4242)\n",
            "pixel maximo:  tensor(2.8215)\n",
            "pixel minimo:  tensor(-0.4242)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Construyo mi CNN"
      ],
      "metadata": {
        "id": "hLDYgFptiqPd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY0TN4erDxRd"
      },
      "source": [
        "#### Bloque de convolución\n",
        "\n",
        "defino primero un \"bloque\" de una capa CNN\n",
        "construido con los bloques funcionales vistos en clase\n",
        "\n",
        "argumentos a pasar a la función:\n",
        "\n",
        "  - `c_in`:   canales (kernels) de entrada\n",
        "  - `c_out`:  canales (kernels) de salida\n",
        "  - `k`:      tamaño del kernel kxk\n",
        "  - `p`:      tamaño del padding de la convolución\n",
        "  - `s`:      stride de la convolución\n",
        "  - `pk`:     tamaño del kernel del pooling\n",
        "\n",
        "\n",
        "la función pooling se elige directamente dentro del bloque!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bloque de convolución para emplear en mi red\n",
        "\n",
        "def conv_block(c_in, c_out, k=3, p='same', s=1, pk=2):\n",
        "    return torch.nn.Sequential(                               # el módulo Sequential se engarga de hacer el forward de todo lo que tiene dentro.\n",
        "        torch.nn.Conv2d(c_in, c_out, k, padding=p, stride=s), # conv\n",
        "        torch.nn.Tanh(),                                      # activation\n",
        "        torch.nn.MaxPool2d(pk)                                # pooling\n",
        "    )\n"
      ],
      "metadata": {
        "id": "qMxa2DAsim9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Red convolucional (modelo)\n",
        "\n",
        "\n",
        "Ahora SI construyo mi red... usando la clase CNN de pytorch\n",
        "revisar muy bien las dimensiones a emplear en cada capa y\n",
        "tener presente la reducción de las dimensiones.\n",
        "\n",
        "En la útlima capa fully conected `fc`, hacer bien el cálculo final del\n",
        "tamaño del array que se obtiene siguiendo la formula vista en la teoria\n",
        "tanto para la capa conv como para la capa pooling."
      ],
      "metadata": {
        "id": "PgPfrY8VivH8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrO5gfEL3KRC"
      },
      "source": [
        "class CNN(torch.nn.Module):\n",
        "  def __init__(self, n_channels=1, n_outputs=10):\n",
        "    super().__init__()\n",
        "    self.conv1 = conv_block(c_in = n_channels, c_out = 4, k=3, p='same', s=1, pk=2)\n",
        "    self.conv1_out = None\n",
        "    self.drop = torch.nn.Dropout2d(p=0.7, inplace=False)\n",
        "    self.conv2 = conv_block(c_in = 4, c_out = 8, k=3, p='same', s=1, pk=2)\n",
        "    self.conv2_out = None\n",
        "    self.fc = torch.nn.Linear(8*7*7, n_outputs) # verificar la dim de la salida para calcular el tamaño de la fully conected!!\n",
        "\n",
        "\n",
        "    print('Red creada')\n",
        "    print('arquitectura:')\n",
        "    print(self)\n",
        "\n",
        "    # Me fijo en el número de capas\n",
        "    i=0\n",
        "    for layer in self.children():\n",
        "        i=i+1\n",
        "    print('Número total de capas de CNN (conv+act+polling) + finales : ', i)\n",
        "\n",
        "    # Me fijo en el número de parámetros entrenables\n",
        "    pytorch_total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "    print('Número total de parámetros a entrenar: ', pytorch_total_params)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    #print('input shape: ', x.shape)\n",
        "    self.conv1_out = self.drop(self.conv1(x))\n",
        "    self.conv2_out = self.drop(self.conv2(self.conv1_out))\n",
        "    y = self.conv2_out.flatten(start_dim=1)\n",
        "    #print(y.shape)\n",
        "    y = self.fc(y)\n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb6DoGaP31md",
        "outputId": "d29c498d-7bb0-4a13-aea0-20a181cdec66"
      },
      "source": [
        "model = CNN()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Red creada\n",
            "arquitectura:\n",
            "CNN(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (1): Tanh()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (drop): Dropout2d(p=0.7, inplace=False)\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (1): Tanh()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc): Linear(in_features=392, out_features=10, bias=True)\n",
            ")\n",
            "Número total de capas de CNN (conv+act+polling) + finales :  4\n",
            "Número total de parámetros a entrenar:  4266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4tEn-XqHVZ7"
      },
      "source": [
        "## 4. Veamos que las dimensiones sean consistentes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torchinfo.summary(model, input_size=( 12, 1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q44d3Ftwokla",
        "outputId": "8dac78b1-22a8-49cb-8439-447f47ae557a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "CNN                                      [12, 10]                  --\n",
              "├─Sequential: 1-1                        [12, 4, 14, 14]           --\n",
              "│    └─Conv2d: 2-1                       [12, 4, 28, 28]           40\n",
              "│    └─Tanh: 2-2                         [12, 4, 28, 28]           --\n",
              "│    └─MaxPool2d: 2-3                    [12, 4, 14, 14]           --\n",
              "├─Dropout2d: 1-2                         [12, 4, 14, 14]           --\n",
              "├─Sequential: 1-3                        [12, 8, 7, 7]             --\n",
              "│    └─Conv2d: 2-4                       [12, 8, 14, 14]           296\n",
              "│    └─Tanh: 2-5                         [12, 8, 14, 14]           --\n",
              "│    └─MaxPool2d: 2-6                    [12, 8, 7, 7]             --\n",
              "├─Dropout2d: 1-4                         [12, 8, 7, 7]             --\n",
              "├─Linear: 1-5                            [12, 10]                  3,930\n",
              "==========================================================================================\n",
              "Total params: 4,266\n",
              "Trainable params: 4,266\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 1.12\n",
              "==========================================================================================\n",
              "Input size (MB): 0.04\n",
              "Forward/backward pass size (MB): 0.45\n",
              "Params size (MB): 0.02\n",
              "Estimated Total Size (MB): 0.51\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoB3GvDtGUgY"
      },
      "source": [
        "## 5. Armo las funciones necesarias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fM09lfI74jQ"
      },
      "source": [
        "# función de entrenamiento\n",
        "def fit(model, dataloader, epochs=15):\n",
        "    # enviamos el modelo al device\n",
        "    model.to(device)\n",
        "    # definimo optimizer y la función de pérdida\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    # preparamos listas para guardar las loss y la acc a lo largo de la epocas\n",
        "    epoch_t_loss = []\n",
        "    epoch_v_loss = []\n",
        "    epoch_t_acc = []\n",
        "    epoch_v_acc = []\n",
        "    # iteramos en las epocas\n",
        "    for epoch in range(1, epochs+1):\n",
        "        # ponemos el modelo en train\n",
        "        model.train()\n",
        "        # listas de loss y acc de train para esta epoca\n",
        "        # así despues calculamos la media\n",
        "        # por que el dataset lo pasamos de a batches\n",
        "        train_loss, train_acc = [], []\n",
        "        bar = tqdm(dataloader['train'])\n",
        "        for batch in bar:\n",
        "            X, y = batch  # sacamos X e y del batch\n",
        "            X, y = X.to(device), y.to(device) # lo enviamos al device\n",
        "            optimizer.zero_grad() # llevamos optimizer a zero\n",
        "            y_hat = model(X)  # corremos el modelo y vemos su predicción\n",
        "            loss = criterion(y_hat, y)  # calculamos la pérdida\n",
        "            loss.backward() # back-propagations\n",
        "            optimizer.step()  # step del optimizer\n",
        "            train_loss.append(loss.item()) # vamos guardando la pérdida de este batch, en la perdida de la epoca\n",
        "            # calculo de la acc\n",
        "            acc = (y == torch.argmax(y_hat, axis=1)).sum().item() / len(y)\n",
        "            train_acc.append(acc) # vamos guardando la acc de este batch, en la acc de la epoca\n",
        "            # seteamos descriptores en la barra\n",
        "            bar.set_description(f\"loss {np.mean(train_loss):.5f} acc {np.mean(train_acc):.5f}\")\n",
        "\n",
        "        # luego de pasar todo el batch, guardamos la perdida y acc media del train\n",
        "        epoch_t_loss.append(np.mean(train_loss))\n",
        "        epoch_t_acc.append(np.mean(train_acc))\n",
        "\n",
        "\n",
        "        # ahora viene es test\n",
        "        bar = tqdm(dataloader['test'])\n",
        "        # listas de loss y acc de test para esta epoca\n",
        "        # así despues calculamos la media\n",
        "        # por que el dataset lo pasamos de a batches\n",
        "        val_loss, val_acc = [], []\n",
        "        # ponemos en eval el modelo\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in bar:\n",
        "                X, y = batch\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                y_hat = model(X)\n",
        "                loss = criterion(y_hat, y)\n",
        "                val_loss.append(loss.item())\n",
        "                acc = (y == torch.argmax(y_hat, axis=1)).sum().item() / len(y)\n",
        "                val_acc.append(acc)\n",
        "                bar.set_description(f\"val_loss {np.mean(val_loss):.5f} val_acc {np.mean(val_acc):.5f}\")\n",
        "        print(f\"Epoch {epoch}/{epochs} loss {np.mean(train_loss):.5f} val_loss {np.mean(val_loss):.5f} acc {np.mean(train_acc):.5f} val_acc {np.mean(val_acc):.5f}\")\n",
        "\n",
        "        epoch_v_loss.append(np.mean(val_loss))\n",
        "        epoch_v_acc.append(np.mean(val_acc))\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(epoch_t_loss, 'r')\n",
        "    plt.plot(epoch_v_loss, 'g')\n",
        "    plt.title('loss')\n",
        "    plt.legend(['train loss', 'val loss'])\n",
        "    plt.grid()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(epoch_t_acc, 'r')\n",
        "    plt.plot(epoch_v_acc, 'g')\n",
        "    plt.title('acc')\n",
        "    plt.legend(['train acc', 'val acc'])\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zagkkBhIG9Kc"
      },
      "source": [
        "## 6. Entreno la red"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYHKN7XH7-Kk",
        "outputId": "fd20c139-c003-42d0-90db-86cb9bd24af5"
      },
      "source": [
        "fit(model, dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 2.05428 acc 0.29339: 100%|██████████| 938/938 [00:19<00:00, 47.93it/s]\n",
            "val_loss 1.39334 val_acc 0.77617: 100%|██████████| 157/157 [00:02<00:00, 54.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15 loss 2.05428 val_loss 1.39334 acc 0.29339 val_acc 0.77617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.51278 acc 0.52572: 100%|██████████| 938/938 [00:20<00:00, 46.24it/s]\n",
            "val_loss 0.77813 val_acc 0.83788: 100%|██████████| 157/157 [00:02<00:00, 55.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/15 loss 1.51278 val_loss 0.77813 acc 0.52572 val_acc 0.83788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.28463 acc 0.58452: 100%|██████████| 938/938 [00:19<00:00, 48.14it/s]\n",
            "val_loss 0.59745 val_acc 0.86654: 100%|██████████| 157/157 [00:02<00:00, 55.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/15 loss 1.28463 val_loss 0.59745 acc 0.58452 val_acc 0.86654\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.20716 acc 0.60379: 100%|██████████| 938/938 [00:19<00:00, 48.59it/s]\n",
            "val_loss 0.51683 val_acc 0.87550: 100%|██████████| 157/157 [00:02<00:00, 57.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/15 loss 1.20716 val_loss 0.51683 acc 0.60379 val_acc 0.87550\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.15474 acc 0.61794: 100%|██████████| 938/938 [00:19<00:00, 48.54it/s]\n",
            "val_loss 0.46989 val_acc 0.88097: 100%|██████████| 157/157 [00:03<00:00, 45.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/15 loss 1.15474 val_loss 0.46989 acc 0.61794 val_acc 0.88097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.12811 acc 0.62382: 100%|██████████| 938/938 [00:20<00:00, 46.19it/s]\n",
            "val_loss 0.43992 val_acc 0.88515: 100%|██████████| 157/157 [00:03<00:00, 46.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/15 loss 1.12811 val_loss 0.43992 acc 0.62382 val_acc 0.88515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.10360 acc 0.63151: 100%|██████████| 938/938 [00:19<00:00, 49.08it/s]\n",
            "val_loss 0.41779 val_acc 0.89112: 100%|██████████| 157/157 [00:02<00:00, 55.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/15 loss 1.10360 val_loss 0.41779 acc 0.63151 val_acc 0.89112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.08452 acc 0.63293: 100%|██████████| 938/938 [00:19<00:00, 47.33it/s]\n",
            "val_loss 0.39994 val_acc 0.89291: 100%|██████████| 157/157 [00:02<00:00, 56.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/15 loss 1.08452 val_loss 0.39994 acc 0.63293 val_acc 0.89291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.07585 acc 0.63556: 100%|██████████| 938/938 [00:19<00:00, 48.06it/s]\n",
            "val_loss 0.38648 val_acc 0.89480: 100%|██████████| 157/157 [00:02<00:00, 56.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/15 loss 1.07585 val_loss 0.38648 acc 0.63556 val_acc 0.89480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.06010 acc 0.64131: 100%|██████████| 938/938 [00:18<00:00, 49.42it/s]\n",
            "val_loss 0.37732 val_acc 0.89729: 100%|██████████| 157/157 [00:03<00:00, 48.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/15 loss 1.06010 val_loss 0.37732 acc 0.64131 val_acc 0.89729\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.05064 acc 0.64292: 100%|██████████| 938/938 [00:18<00:00, 49.66it/s]\n",
            "val_loss 0.36624 val_acc 0.89918: 100%|██████████| 157/157 [00:03<00:00, 51.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/15 loss 1.05064 val_loss 0.36624 acc 0.64292 val_acc 0.89918\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.05127 acc 0.64214: 100%|██████████| 938/938 [00:19<00:00, 48.02it/s]\n",
            "val_loss 0.35971 val_acc 0.90078: 100%|██████████| 157/157 [00:02<00:00, 55.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/15 loss 1.05127 val_loss 0.35971 acc 0.64214 val_acc 0.90078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.03857 acc 0.64735:  28%|██▊       | 262/938 [00:05<00:14, 45.15it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHcr6aDtHNfc"
      },
      "source": [
        "## 7. Vemos que funcione."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_jFvJ603PC-"
      },
      "source": [
        "# corremos 1 dato, a ver como lo clasifica...\n",
        "# generamos un batch del dataloader\n",
        "test_features, test_labels = next(iter(dataloader['test']))\n",
        "\n",
        "# item a usar k\n",
        "k = 2\n",
        "\n",
        "# verifico las dimensiones y los valores que toma algun pixel.\n",
        "samp_img = test_features[k]\n",
        "print(samp_img.shape)\n",
        "print(samp_img[0][0][0])\n",
        "print(torch.max(samp_img))\n",
        "print(torch.min(samp_img))\n",
        "\n",
        "# ploteo la imagen\n",
        "plt.imshow(samp_img.squeeze(), cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "# preparo para pasarla a la red (model) asi predice.\n",
        "samp_imp = samp_img.unsqueeze(0) # agrego la batch dim\n",
        "samp_img = samp_img.unsqueeze(0).to(device)\n",
        "print('Tamaño imagen de entrada a red: ', samp_img.shape)\n",
        "\n",
        "# la paso al modelo\n",
        "model.to(device)\n",
        "model.eval()\n",
        "y_hat = model(samp_img)\n",
        "print('Predición del modelo:')\n",
        "print(y_hat.detach())\n",
        "print()\n",
        "print('softmax de predicción:')\n",
        "print(torch.nn.functional.softmax(y_hat, dim=1).detach())\n",
        "print()\n",
        "print(f'El numero es un: ', torch.argmax(y_hat, axis=1).item())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZM8DWqENNDh"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# EJERCICIO NRO 1\n",
        "\n",
        "## Probemos con un número nuestro!\n",
        "Primero lo dibujamos en el paint con un tamaño próximo a 28x28 pixeles\n",
        "(el tamaño del papel puede ser aproximado)...\n",
        "Después haremos un `resize`.\n",
        "Usar un marcador grueso que pinte varios pixeles!!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAQboG3CIILq"
      },
      "source": [
        "Cargamos el archivo hecho a mano"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju1kgUR0GUE_"
      },
      "source": [
        "# lo leemos\n",
        "num_ex = torchvision.io.read_image('cinco.png')\n",
        "#num_ex= torchvision.io.read_image('/content/drive/My Drive/CIA_marcos/deep_learning/clase_5/tres.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qi4mD6qIKon"
      },
      "source": [
        "Veamos el tamaño, datatype y el valor de 1 pixel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNQ4v03sGWVx"
      },
      "source": [
        "print('shape imagen: ', num_ex.shape)\n",
        "print('data type imagen: ', num_ex.dtype)\n",
        "# convertimos a float\n",
        "num_ex = num_ex.float()\n",
        "print('nuevo data type: ', num_ex.dtype)\n",
        "print('valor pixel superior izq: ', num_ex[0][0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjEbxOyxIZ10"
      },
      "source": [
        "Realizamos un conjunto de transformaciones para llevar la imagen al valor deseado:\n",
        "- resize a 28x28\n",
        "- convertir a escala de grises\n",
        "- escalar ente 0 y 1 (dividimos por 255)\n",
        "- normalizar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyWn2dmyJVLn"
      },
      "source": [
        "adaptacion = torchvision.transforms.Compose([\n",
        "                                             torchvision.transforms.Resize(size=(28,28)),\n",
        "                                             torchvision.transforms.Grayscale(num_output_channels=1), #<---------- IMPORTANTE!\n",
        "                                             torchvision.transforms.Normalize((0,), (255,))#,#<--------- lo escalo entre 0 y 1\n",
        "                                             #torchvision.transforms.Normalize((0.1307,), (0.3081,)), #<---------- IMPORTANTE!\n",
        "                                            ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una ultima transformación (que no se pudo colocar dentro del compose) es la de invertir los colores, ya que MNIST tiene fondo negro con letra blanca."
      ],
      "metadata": {
        "id": "gh6EGzccktGH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5yVpzjgKSD-"
      },
      "source": [
        "# aplicamos la transformación\n",
        "num_ex_adap = adaptacion(num_ex)\n",
        "\n",
        "# para invertir el color (blanco y negro)\n",
        "num_ex_adap = torchvision.transforms.functional.invert(num_ex_adap)\n",
        "\n",
        "print('nuevo tamaño de imagen: ', num_ex_adap.shape)\n",
        "print('pixel [0,0]: ',num_ex_adap[0][0][0])\n",
        "print('pixel máximo: ', torch.max(num_ex_adap))\n",
        "print('pixel mínimo: ', torch.min(num_ex_adap))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOaOA8JYIioV"
      },
      "source": [
        "Veamos como quedó"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2h5r-FqKz6D"
      },
      "source": [
        "# ponemos el squeeze para eliminar el canal (1ra dimension)\n",
        "plt.imshow(num_ex_adap.squeeze(), cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaiEjwycIlh9"
      },
      "source": [
        "Lo pasamos por la red...\n",
        "RECORDAR DE MANEJAR TODO EN EL MISMO `DEVICE`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFVUKwe0MxiE"
      },
      "source": [
        "# le agregamos el batch y lo mandamos a device\n",
        "num_ex_adap_batch = num_ex_adap.unsqueeze(0).cpu()\n",
        "print('Tamaño entrada:   ',num_ex_adap_batch.shape)\n",
        "\n",
        "# enviamos al modelo al device\n",
        "model.cpu()\n",
        "model.eval()\n",
        "# pasamos la entrada al modelo\n",
        "y_hat = model(num_ex_adap_batch)\n",
        "print()\n",
        "print('Salida red:  ')\n",
        "print(y_hat.detach())\n",
        "print()\n",
        "print('Salida softmax(salida red):')\n",
        "print(torch.nn.functional.softmax(y_hat, dim=1).detach())\n",
        "print()\n",
        "print(f'El numero es un: ', torch.argmax(y_hat, axis=1).item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El mismo proceso en 1 sola celda."
      ],
      "metadata": {
        "id": "xBd1u15ArKqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# todo junto\n",
        "num_ex = torchvision.io.read_image('ocho.png').float()\n",
        "num_ex_adap = adaptacion(num_ex)\n",
        "num_ex_adap = torchvision.transforms.functional.invert(num_ex_adap)\n",
        "# le agregamos el batch y lo mandamos a device\n",
        "num_ex_adap_batch = num_ex_adap.unsqueeze(0).cpu()\n",
        "model.cpu().eval()\n",
        "\n",
        "y_hat = model(num_ex_adap_batch)\n",
        "print(f'El numero es un: ', torch.argmax(y_hat, axis=1).item())"
      ],
      "metadata": {
        "id": "pqMeWUIymQ0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LSTRStMDd5vu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}